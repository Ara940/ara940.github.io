@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}



%%%%%%%%%%%%%%%% Diffusion models
@misc{ling2022diffusionsurvey,
  url = {https://arxiv.org/abs/2209.00796},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
  year = {2022},
}

@inproceedings{ho2020ddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 year = {2020}
}

@InProceedings{sohldickstein2015deepunsupervised,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  year = 	 {2015},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}
@inproceedings{karras2022edm,
  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022}
}

@article{song2017pixeldefend,
  title   = {Pixeldefend: Leveraging generative models to understand and defend against adversarial examples},
  author  = {Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
  journal = {arXiv preprint arXiv:1710.10766},
  year    = {2017}
}
@inproceedings{song2019generative,
  title     = {Generative modeling by estimating gradients of the data distribution},
  author    = {Song, Yang and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {32},
  year      = {2019}
}
@inproceedings{song2019sliced,
  author    = {Yang Song and
               Sahaj Garg and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {204},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/204.pdf}
}
@inproceedings{song2021ddim,
  title     = {Denoising Diffusion Implicit Models},
  author    = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021}
}
@inproceedings{song2020improved,
  title     = {Improved techniques for training score-based generative models},
  author    = {Song, Yang and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {33},
  pages     = {12438--12448},
  year      = {2020}
}
@inproceedings{song2020scoreSDE,
  title     = {Score-Based Generative Modeling through Stochastic Differential Equations},
  author    = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}
@inproceedings{song2021maximum,
  title     = {Maximum likelihood training of score-based diffusion models},
  author    = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {34},
  pages     = {1415--1428},
  year      = {2021}
}
@inproceedings{song2021solving,
  title     = {Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},
  author    = {Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}
@inproceedings{lou2023reflected,
      title={Reflected Diffusion Models},
      author={Aaron Lou and Stefano Ermon},
      booktitle={International Conference on Machine Learning},
      year={2023},
}
@article{song2021train,
  title   = {How to train your energy-based models},
  author  = {Song, Yang and Kingma, Diederik P},
  journal = {arXiv preprint arXiv:2101.03288},
  year    = {2021}
}
@article{song2022applying,
  title   = {Applying Regularized {S}chr{รถ}dinger-Bridge-Based Stochastic Process in Generative Modeling},
  author  = {Song, Ki-Ung},
  journal = {arXiv preprint arXiv:2208.07131},
  year    = {2022}
}

@inproceedings{bansal2022cold,
  title={Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  author={Arpit Bansal and Eitan Borgnia and Hong-Min Chu and Jie S. Li and Hamid Kazemi and Furong Huang and Micah Goldblum and Jonas Geiping and Tom Goldstein},
  year={2023},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  url={https://arxiv.org/abs/2208.09392}
}

%%%%%%%%%%% Video diffusion models
@inproceedings{voleti2022mcvd,
  title={MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation},
  url = {https://arxiv.org/abs/2205.09853},
  author = {Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2022}
}

@article{ho2022videodiffusion,
    title={Video diffusion models},
    author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
    journal={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2022}}
}

@article{singer2022makeavideo,
  url = {https://arxiv.org/abs/2209.14792},
  author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},
  year = {2022}
}

@article{ho2022imagenvideo,
  url = {https://arxiv.org/abs/2210.02303},
  author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
  title = {Imagen Video: High Definition Video Generation with Diffusion Models},
  year = {2022}
}
@article{harvey2022flexiblevideos,
  title   = {Flexible Diffusion Modeling of Long Videos},
  author  = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year    = {2022}
}

@article{yang2022diffusion,
  title   = {Diffusion probabilistic modeling for video generation},
  author  = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
  journal = {arXiv preprint arXiv:2203.09481},
  year    = {2022}
}


%%%%%%%%%%%% DATA
@article{Rasp2020weatherbench,
	url = {https://doi.org/10.1029\%2F2020ms002203},
	year = {2020},
	author = {Stephan Rasp and Peter D. Dueben and Sebastian Scher and Jonathan A. Weyn and Soukayna Mouatadid and Nils Thuerey},
	title = {{WeatherBench}: A Benchmark Data Set for Data-Driven Weather Forecasting},
	journal = {Journal of Advances in Modeling Earth Systems}
}
@article{otness21nnbenchmark,
  title={An Extensible Benchmark Suite for Learning to Simulate Physical Systems},
  author={Karl Otness and Arvi Gjoka and Joan Bruna and Daniele Panozzo and Benjamin Peherstorfer and Teseo Schneider and Denis Zorin},
  year={2021},
  url={https://arxiv.org/abs/2108.07799},
  journal={Advances in Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks}
}

@article{huang2021oisstv2,
      author = "Boyin Huang and Chunying Liu and Viva Banzon and Eric Freeman and Garrett Graham and Bill Hankins and Tom Smith and Huai-Min Zhang",
      title = "Improvements of the Daily Optimum Interpolation Sea Surface Temperature (DOISST) Version 2.1",
      journal = "Journal of Climate",
      year = "2021",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "34",
      number = "8",
      doi = "https://doi.org/10.1175/JCLI-D-20-0166.1",
      pages=      "2923 - 2939",
      url = "https://journals.ametsoc.org/view/journals/clim/34/8/JCLI-D-20-0166.1.xml"
}

%  Deep learning weather forecasting
@article{pathak2022fourcastnet,
  url = {https://arxiv.org/abs/2202.11214},
  author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
  title = {{FourCastNet}: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators},
  year = {2022},
}   % At short term similarly good or better than IFS. Training is done on next-step predictions, and then fine-tuned on unrolled step 1 and 2 predictions

@article{lam2022graphcast,
  url = {https://arxiv.org/abs/2212.12794},
  author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Stott, Jacklynn and Vinyals, Oriol and Mohamed, Shakir and Battaglia, Peter},
  title = {{GraphCast}: Learning skillful medium-range global weather forecasting},
  year = {2022},
}

@article{bi2022pangu,
  title={Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={arXiv preprint arXiv:2211.02556},
  year={2022}
} % Hierarchical lead-time models so that mid-range forecasts require few NN calls. Ensembling via input perturbations like fourcastnet

@article{nguyen2023climax,
  title={Clima{X}: A foundation model for weather and climate},
  author={Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K and Grover, Aditya},
  journal={International Conference on Machine Learning},
  year={2023}
}