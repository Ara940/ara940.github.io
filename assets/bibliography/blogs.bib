% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.
@STRING{ICML = {International Conference on Machine Learning}}
@STRING{NIPS = {Advances in Neural Information Processing Systems}}
@STRING{CVPR = {IEEE Conference on Computer Vision and Pattern Recognition}}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@inproceedings{cachay2023dyffusion,
  title={{DYffusion:} A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting},
  author={R{\"u}hling Cachay, Salva and Zhao, Bo and Joren, Hailey and Yu, Rose},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  abbr={NeurIPS},
  abstract={While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for efficiently training diffusion models for probabilistic spatiotemporal forecasting, where generating stable and accurate rollout forecasts remains challenging, Our method, DYffusion, leverages the temporal dynamics in the data, directly coupling it with the diffusion steps in the model. We train a stochastic, time-conditioned interpolator and a forecaster network that mimic the forward and reverse processes of standard diffusion models, respectively. DYffusion naturally facilitates multi-step and long-range forecasting, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process in DYffusion imposes a strong inductive bias and significantly improves computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic forecasting of complex dynamics in sea surface temperatures, Navier-Stokes flows, and spring mesh systems.},
  pdf={https://arxiv.org/abs/2306.01984},
  OpenReview={https://openreview.net/forum?id=WRGldGm5Hz},
  code={https://github.com/Rose-STL-Lab/dyffusion},
  poster={https://salvarc.github.io/posters/dyffusion.pdf},
  slides={https://docs.google.com/presentation/d/e/2PACX-1vQqDOEW5CQWIafaTShAGzAfcbgvAH6t5YPUqTZTPIzToWDfFX776Xtz4Ly31b9654H5uA8dDJGve6aW/pub?start=false&loop=false&delayms=15000&slide=id.p},
  year={2023},
  selected={true}
}

@inproceedings{cachay2021climart,
    title={{ClimART}: A Benchmark Dataset for Emulating Atmospheric Radiative Transfer in Weather and Climate Models},
    author={R{\"u}hling Cachay*, Salva and Ramesh*, Venkatesh and Cole, Jason N. S. and Barker, Howard and Rolnick, David},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    abbr={NeurIPS},
    abstract={Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. This has made them a popular target for neural network-based emulators. However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking. To fill this gap, we build a large dataset, ClimART, with more than \emph{10 million samples from present, pre-industrial, and future climate conditions}, based on the Canadian Earth System Model. ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed. We also present several novel baselines that indicate shortcomings of datasets and network architectures used in prior work.},
    pdf={https://arxiv.org/abs/2111.14671},
    OpenReview={https://openreview.net/forum?id=FZBtIpEAb5J},
    code={https://github.com/RolnickLab/climart},
    poster={https://salvarc.github.io/posters/climart.png},
    slides={https://salvarc.github.io/slides/ClimART.pdf},
    year={2021},
    selected={true}
}

@inproceedings{cachay2021endtoend,
  title={End-to-End Weak Supervision},
  author={R{\"u}hling Cachay, Salva and Boecking, Benedikt and Dubrawski, Artur},
  booktitle={Advances in Neural Information Processing Systems},
  abbr={NeurIPS},
  abstract={Aggregating multiple sources of weak supervision (WS) can ease the data-labeling bottleneck prevalent in many machine learning applications, by replacing the tedious manual collection of ground truth labels. Current state of the art approaches that do not use any labeled training data, however, require two separate modeling steps: Learning a probabilistic latent variable model based on the WS sources -- making assumptions that rarely hold in practice -- followed by downstream model training. Importantly, the first step of modeling does not consider the performance of the downstream model. To address these caveats we propose an end-to-end approach for directly learning the downstream model by maximizing its agreement with probabilistic labels generated by reparameterizing previous probabilistic posteriors with a neural network. Our results show improved performance over prior work in terms of end model performance on downstream test sets, as well as in terms of improved robustness to dependencies among weak supervision sources.},
  pdf={https://arxiv.org/abs/2107.02233},
  code={https://github.com/autonlab/weasel},
  poster={https://salvarc.github.io/posters/weasel.pdf},
  slides={https://salvarc.github.io/slides/WeaSEL.pdf},
  year={2021},
  selected={true}
}

@article{cachay2021world,
    title={The World as a Graph: Improving El Ni\~no Forecasts with Graph Neural Networks},
    author={R{\"u}hling Cachay, Salva and Erickson, Emma and  Fender C. Bucker, Arthur and Pokropek, Ernest and Potosnak, Willa and Bire, Suyash and Osei, Salomey and Björn Lütjens},
    journal={arXiv:2310.14189},
    abbr={arXiv},
    abstract={Deep learning-based models have recently outperformed state-of-the-art seasonal forecasting models, such as for predicting El Ni~no-Southern Oscillation (ENSO). However, current deep learning models are based on convolutional neural networks which are difficult to interpret and can fail to model large-scale atmospheric patterns. In comparison, graph neural networks (GNNs) are capable of modeling large-scale spatial dependencies and are more interpretable due to the explicit modeling of information flow through edge connections. We propose the first application of graph neural networks to seasonal forecasting. We design a novel graph connectivity learning module that enables our GNN model to learn large-scale spatial interactions jointly with the actual ENSO forecasting task. Our model, \graphino, outperforms state-of-the-art deep learning-based models for forecasts up to six months ahead. Additionally, we show that our model is more interpretable as it learns sensible connectivity structures that correlate with the ENSO anomaly pattern.},
    pdf={https://arxiv.org/abs/2104.05089},
    code={https://github.com/salvaRC/graphino},
    media={https://ai4edatasetspublicassets.blob.core.windows.net/grantee-profiles/Salva%20Ruhling%20Cachay_EMEA_Climate_AI4E%20Grantee%20Profile_Final.pdf},
    year={2021},
    selected={true}
}
% To reference this in text, use \cite{c1}
@article{DP,
author = {Ratner, Alexander and De Sa, Christopher and Wu, Sen and Selsam, Daniel and Ré, Christopher},
year = {2016},
month = {05},
pages = {},
title = {Data Programming: Creating Large Training Sets, Quickly},
volume = {29},
journal = {Advances in neural information processing systems}
}

@article{Snorkel,
author = {Ratner, Alexander and Bach, Stephen and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
year = {2019},
month = {07},
pages = {},
title = {Snorkel: rapid training data creation with weak supervision},
volume = {29},
journal = {The VLDB Journal},
doi = {10.1007/s00778-019-00552-1}
}

@article{dunnmon2020cross,
  title={Cross-modal data programming enables rapid medical machine learning},
  author={Dunnmon, Jared A and Ratner, Alexander J and Saab, Khaled and Khandwala, Nishith and Markert, Matthew and Sagreiya, Hersh and Goldman, Roger and Lee-Messer, Christopher and Lungren, Matthew P and Rubin, Daniel L and others},
  journal={Patterns},
  volume={1},
  number={2},
  pages={100019},
  year={2020},
  publisher={Elsevier}
}

@article{fries2019weakly,
  title={Weakly supervised classification of aortic valve malformations using unlabeled cardiac MRI sequences},
  author={Fries, Jason A and Varma, Paroma and Chen, Vincent S and Xiao, Ke and Tejeda, Heliodoro and Saha, Priyanka and Dunnmon, Jared and Chubb, Henry and Maskatia, Shiraz and Fiterau, Madalina and others},
  journal={Nature Communications},
  volume={10},
  number={1},
  pages={1--10},
  year={2019},
  publisher={Nature Publishing Group}
}

@misc{Git-Snorkel,
url = "https://github.com/snorkel-team/snorkel"
}

@article{TripletsMean,
    author = {Chen, Mayee F. and Cohen-Wang, Benjamin and Mussmann, Steve and Sala, Frederic and Ré, Christopher},
    year = {2021},
    title = {Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments Latent Variable Estimation.},
    journal = {AISTATS}
}

@article{MisspecificationInDP,
  author = {{\color{blue}Salva R{\"{u}}hling Cachay} and
               Benedikt Boecking and
               Artur Dubrawski},
  title = {Dependency Structure Misspecification in    Multi-Source Weak Supervision Models},
   year = {2021},
   journal = {ICLR Workshop on Weakly Supervised Learning}
}

@article{MisspecificationInDPLXAI,
  author = {{\color{blue}Salva R{\"{u}}hling Cachay} and
               Benedikt Boecking and
               Artur Dubrawski},
  title = {Dependency Misspecification in Multiple Weak Supervision},
   year = {2020},
   journal = {NeurIPS LatinX in AI workshop}
}

@article{cachay2021world,
      title={The World as a Graph: Improving {El Ni\~no} Forecasts with Graph Neural Networks},
      author={{\color{blue}Salva Rühling Cachay} and Emma Erickson and Arthur Fender C. Bucker and Ernest Pokropek and Willa Potosnak and Suyash Bire and Salomey Osei and Björn Lütjens},
      year={2021},
      eprint={2104.05089},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
          journal = {IEEE TNNLS (under review)}
}

@inproceedings{co_training,
    author = {Blum, Avrim and Mitchell, Tom},
    title = {Combining Labeled and Unlabeled Data with Co-Training},
    year = {1998},
    isbn = {1581130570},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/279943.279962},
    booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
    pages = {92–100},
    numpages = {9},
    series = {COLT' 98}
}



@inproceedings{Awasthi2020Learning,
    title={Learning from Rules Generalizing Labeled Exemplars},
    author={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},
    booktitle={ICLR},
    year={2020}
}

@InProceedings{astra,
    author = {Karamanolakis, Giannis and Mukherjee, Subhabrata (Subho) and Zheng, Guoqing and Awadallah, Ahmed H.},
    title = {Self-training with Weak Supervision},
    booktitle = {NAACL},
    year = {2021}
}

@inproceedings{Drybell,
author = {Bach, Stephen H. and Rodriguez, Daniel and Liu, Yintao and Luo, Chong and Shao, Haidong and Xia, Cassandra and Sen, Souvik and Ratner, Alex and Hancock, Braden and Alborzi, Houman and Kuchhal, Rahul and R\'{e}, Chris and Malkin, Rob},
title = {Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3299869.3314036},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {362–375},
numpages = {14},
keywords = {weak supervision, systems for machine learning},
location = {Amsterdam, Netherlands},
series = {SIGMOD ’19}
}



@inproceedings{structureLearning1,
    author = {Bach, Stephen H. and He, Bryan and Ratner, Alexander and R\'{e}, Christopher},
    title = {Learning the Structure of Generative Models without Labeled Data},
    year = {2017},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
    pages = {273–282},
    numpages = {10},
    location = {Sydney, NSW, Australia},
    series = {ICML'17}
}


@inproceedings{structureLearning2,
  title={Learning dependency structures for weak supervision models},
  author={Varma, Paroma and Sala, Frederic and He, Ann and Ratner, Alexander and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={6418--6427},
  year={2019},
  organization={PMLR}
}

@incollection{structureLearning2Theory,
title = {Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses},
author = {Loh, Po-ling and Wainwright, Martin J},
booktitle = {Advances in Neural Information Processing Systems 25},
pages = {2087--2095},
year = {2012},
publisher = {Curran Associates, Inc.}
}

@inproceedings{SL-static-analysis,
  title={Inferring generative model structure with static analysis},
  author={Varma, Paroma and He, Bryan D and Bajaj, Payal and Khandwala, Nishith and Banerjee, Imon and Rubin, Daniel and R{\'e}, Christopher},
  booktitle={Advances in neural information processing systems},
  pages={240--250},
  year={2017}
}

@article{Multitask,
author = {Ratner, Alexander and Hancock, Braden and Dunnmon, Jared and Sala, Frederic and Pandey, Shreyash and Ré, Christopher},
year = {2019},
month = {07},
pages = {4763-4771},
title = {Training Complex Models with Multi-Task Weak Supervision},
volume = {33},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v33i01.33014763}
}

@article{triplets,
  title={Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods},
  author={Daniel Y. Fu and Mayee F. Chen and Frederic Sala and Sarah Hooper and Kayvon Fatahalian and Christopher R{\'e}},
  journal={ICML},
  year={2020}
}

@inproceedings{Snuba,
  title={Snuba: Automating weak supervision to label training data},
  author={Varma, Paroma and R{\'e}, Christopher},
  booktitle={Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases},
  pages={223},
  year={2018},
  organization={NIH Public Access}
}

@article{chatterjee2019data,
  title={Data Programming using Continuous and Quality-Guided Labeling Functions},
  author={Chatterjee, Oishik and Ramakrishnan, Ganesh and Sarawagi, Sunita},
  journal={AAAI},
  year={2020}
}
@article{anandkumar2014tensor,
  title={Tensor decompositions for learning latent variable models},
  author={Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M and Telgarsky, Matus},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={2773--2832},
  year={2014},
  publisher={Journal of Machine Learning Research}
}


@InProceedings{distantSupervision1,
author="Riedel, Sebastian
and Yao, Limin
and McCallum, Andrew",
title="Modeling Relations and Their Mentions without Labeled Text",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="148--163",
isbn="978-3-642-15939-8"
}

@inproceedings{distantSupervision2,
  title={Distant supervision for relation extraction without labeled data},
  author={Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  pages={1003--1011},
  year={2009}
}

@inproceedings{Crowdsourcing1,
 author = {Karger, David and Oh, Sewoong and Shah, Devavrat},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1953--1961},
 publisher = {Curran Associates, Inc.},
 title = {Iterative Learning for Reliable Crowdsourcing Systems},
 volume = {24},
 year = {2011}
}


@article{Crowdsourcing2,
  title={Spectral methods meet EM: A provably optimal algorithm for crowdsourcing},
  author={Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={3537--3580},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{Crowdsourcing3,
  title={Aggregating crowdsourced binary ratings},
  author={Dalvi, Nilesh and Dasgupta, Anirban and Kumar, Ravi and Rastogi, Vibhor},
  booktitle={Proceedings of the 22nd international conference on World Wide Web},
  pages={285--294},
  year={2013}
}

@inproceedings{Crowdsourcing4,
 author = {Welinder, Peter and Branson, Steve and Perona, Pietro and Belongie, Serge},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2424--2432},
 publisher = {Curran Associates, Inc.},
 title = {The Multidimensional Wisdom of Crowds},
 volume = {23},
 year = {2010}
}

@article{SkeneModel,
 ISSN = {00359254, 14679876},
 abstract = {In compiling a patient record many facets are subject to errors of measurement. A model is presented which allows individual error-rates to be estimated for polytomous facets even when the patient's "true" response is not available. The EM algorithm is shown to provide a slow but sure way of obtaining maximum likelihood estimates of the parameters of interest. Some preliminary experience is reported and the limitations of the method are described.},
 author = {A. P. Dawid and A. M. Skene},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {20--28},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm},
 volume = {28},
 year = {1979}
}

@inproceedings{patternbasedWS,
  title={Improved pattern learning for bootstrapped entity extraction},
  author={Gupta, Sonal and Manning, Christopher D},
  booktitle={Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
  pages={98--108},
  year={2014}
}

@inproceedings{rulebasedWSforChemistry,
  title={Using rule-based labels for weak supervised learning: a ChemNet for transferable chemical property prediction},
  author={Goh, Garrett B and Siegel, Charles and Vishnu, Abhinav and Hodas, Nathan},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={302--310},
  year={2018}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% COMBINE DENOISING WITH END MODEL
@article{raykar10a,
  author  = {Vikas C. Raykar and Shipeng Yu and Linda H. Zhao and Gerardo Hermosillo Valadez and Charles Florin and Luca Bogoni and Linda Moy},
  title   = {Learning From Crowds},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {43},
  pages   = {1297-1322},
}


@inproceedings{doctorNet,
  title={Who said what: Modeling individual labelers improves classification},
  author={Guan, Melody and Gulshan, Varun and Dai, Andrew and Hinton, Geoffrey},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{aggNet,
  author={S. {Albarqouni} and C. {Baur} and F. {Achilles} and V. {Belagiannis} and S. {Demirci} and N. {Navab}},
  journal={IEEE Transactions on Medical Imaging},
  title={AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images},
  year={2016},
  volume={35},
  number={5},
  pages={1313-1321},
}

@article{khetan2017learning,
  title={Learning from noisy singly-labeled data},
  author={Khetan, Ashish and Lipton, Zachary C and Anandkumar, Anima},
  journal={ICLR},
  year={2018}
}

@inproceedings{rodrigues2018deep,
  title={Deep learning from crowds},
  author={Rodrigues, Filipe and Pereira, Francisco},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{MaxMIG,
  title={Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds},
  author={Cao, Peng and Xu, Yilun and Kong, Yuqing and Wang, Yizhou},
  journal={ICLR},
  year={2019}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{dehghani2017neural,
author = {Dehghani, Mostafa and Zamani, Hamed and Severyn, Aliaksei and Kamps, Jaap and Croft, W. Bruce},
title = {Neural Ranking Models with Weak Supervision},
year = {2017},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {65–74},
numpages = {10},
location = {Shinjuku, Tokyo, Japan}
}


@inproceedings{zamani2018neural,
author = {Zamani, Hamed and Croft, W. Bruce and Culpepper, J. Shane},
title = {Neural Query Performance Prediction Using Weak Supervision from Multiple Signals},
year = {2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {105–114},
numpages = {10},
keywords = {query performance prediction, neural networks, weak supervision, deep learning, quality estimation},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}


@inproceedings{zamani2018theory,
author = {Zamani, Hamed and Croft, W. Bruce},
title = {On the Theory of Weak Supervision for Information Retrieval},
year = {2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {147–154},
numpages = {8},
location = {Tianjin, China}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{IWS,
      title={Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling},
      author={Benedikt Boecking and Willie Neiswanger and Eric Xing and Artur Dubrawski},
      year={2021},
      journal={ICLR}
}

%%%%%%%%%%%%%%%%%% DATASETS
@inproceedings{IMDB,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and Daly, Raymond E.  and Pham, Peter T.  and Huang, Dan  and Ng, Andrew Y.  and Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    pages = "142--150",
}

@inproceedings{biasBios,
  title={Bias in bios: A case study of semantic representation bias in a high-stakes setting},
  author={De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={120--128},
  year={2019}
}

@inproceedings{Amazon,
  title={Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering},
  author={He, Ruining and McAuley, Julian},
  booktitle={Proceedings of the 25th International Conference on World Wide Web},
  pages={507--517},
  year={2016}
}

@article{Spouses,
    title={What do a million news articles look like?},
    author={Corney, David and Albakour, Dyaa and Martinez-Alvarez, Miguel and Moussa, Samir},
    year={2016},
    journal={Workshop on Recent Trends in News Information Retrieval},
    pages={42--47},
}

@article{watson2022climatebench,
  title={ClimateBench v1. 0: A benchmark for data-driven climate projections},
  author={Watson-Parris, Duncan and Rao, Yuhan and Olivi{\'e}, Dirk and Seland, {\O}yvind and Nowack, Peer and Camps-Valls, Gustau and Stier, Philip and Bouabid, Shahine and Dewey, Maura and Fons, Emilie and others},
  journal={Journal of Advances in Modeling Earth Systems},
  pages={e2021MS002954},
  year={2022},
  publisher={Wiley Online Library}
}


%%%%%%%%%%%%%%%%%%% PGM Properties
@inproceedings{PGM_Lipschitz,
author = {Honorio, Jean},
title = {Lipschitz Parametrization of Probabilistic Graphical Models},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the log-likelihood of several probabilistic graphical models is Lipschitz continuous with respect to the ℓp-norm of the parameters. We discuss several implications of Lipschitz parametrization. We present an upper bound of the Kullback-Leibler divergence that allows understanding methods that penalize the ℓp-norm of differences of parameters as the minimization of that upper bound. The expected log-likelihood is lower bounded by the negative ℓp-norm, which allows understanding the generalization ability of probabilistic models. The exponential of the negative ℓp-norm is involved in the lower bound of the Bayes error rate, which shows that it is reasonable to use parameters as features in algorithms that rely on metric spaces (e.g. classification, dimensionality reduction, clustering). Our results do not rely on specific algorithms for learning the structure or parameters. We show preliminary results for activity recognition and temporal segmentation.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {347–354},
numpages = {8},
location = {Barcelona, Spain},
series = {UAI'11}
}

%%%%%%%%%%%%%%%% f-divergences
@article{nguyen2009surrogate,
  title={On surrogate loss functions and f-divergences},
  author={Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael I and others},
  journal={The Annals of Statistics},
  volume={37},
  number={2},
  pages={876--904},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}
%%%%%%%%%%%%%%%%%%% MODEL MISSPECIFICATION
@article{misspecified_MLE,
  title={Maximum likelihood estimation of misspecified models},
  author={White, Halbert},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1--25},
  year={1982},
  publisher={JSTOR}
}
  @inproceedings{misspecified_Gaussian_PGM,
  title={On model misspecification and KL separation for Gaussian graphical models},
  author={Jog, Varun and Loh, Po-Ling},
  booktitle={2015 IEEE International Symposium on Information Theory (ISIT)},
  pages={1174--1178},
  year={2015},
  organization={IEEE}
}



%%%%%%%%%%%%%%% NOISY LABELS
@inproceedings{robustL1loss,
  title={Robust loss functions under label noise for deep neural networks},
  author={Ghosh, Aritra and Kumar, Himanshu and Sastry, PS},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{L1notSoGood,
 author = {Zhang, Zhilu and Sabuncu, Mert},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {8778--8788},
 publisher = {Curran Associates, Inc.},
 title = {Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels},
 volume = {31},
 year = {2018}
}




%%%%%%%%%%%%%%%%%%%%%%%%%% REGRESSION + WS
@InProceedings{regressionCrowdsourcing,
title = {Iterative Bayesian Learning for Crowdsourced Regression},
author = {Ok, Jungseul and Oh, Sewoong and Jang, Yunhun and Shin, Jinwoo and Yi, Yung},
booktitle = {Proceedings of Machine Learning Research},
pages = {1486--1495},
year = {2019},
volume = {89},
series = {Proceedings of Machine Learning Research},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v89/ok19a/ok19a.pdf},
}

@article{distillation,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

%%%%%%%%%%%%%%%%%%%%%%%%% Self- and Semi-Supervised training
%%%%%%%%%%%%%% Agreement maximization
@inproceedings{wei2020combating,
  title={Combating noisy labels by agreement: A joint training method with co-regularization},
  author={Wei, Hongxin and Feng, Lei and Chen, Xiangyu and An, Bo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13726--13735},
  year={2020}
}


@inproceedings{mutualLearning,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4320--4328},
  year={2018}
}  % This paper interleaves the gradient steps of the peer networks

% Semi-Supervised training
@inproceedings{fixMatch,
 author = {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {596--608},
 publisher = {Curran Associates, Inc.},
 title = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
 volume = {33},
 year = {2020}
}
@inproceedings{pseudoEnsembling,
 author = {Bachman, Philip and Alsharif, Ouais and Precup, Doina},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning with Pseudo-Ensembles},
 volume = {27},
 year = {2014}
}


@inproceedings{temporalEnsembling,
  author    = {Samuli Laine and  Timo Aila},
  title     = {Temporal Ensembling for Semi-Supervised Learning},
  booktitle = {ICLR},
  year      = {2017}
}

@inproceedings{meanTeacher,
author = {Tarvainen, Antti and Valpola, Harri},
title = {Mean Teachers Are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {1195–1204},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{metaPseudoLabels,
  title={Meta pseudo labels},
  author={Pham, Hieu and Xie, Qizhe and Dai, Zihang and Le, Quoc V},
  journal={arXiv preprint arXiv:2003.10580},
  year={2020}
}

@article{pseudoLabels,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun},
  journal={Workshop on challenges in representation learning, ICML},
  year={2013}
}

@inproceedings{noisyStudent,
  title={Self-training with noisy student improves imagenet classification},
  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10687--10698},
  year={2020}
}

@inproceedings{arazo2020pseudo,
  title={Pseudo-labeling and confirmation bias in deep semi-supervised learning},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel E and McGuinness, Kevin},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}



%%%%% Self-supervised learning
@inproceedings{simSiam,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15750--15758},
  year={2021}
}
@inproceedings{BYOL,
 author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {21271--21284},
 title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
 volume = {33},
 year = {2020}
}

@article{SimCLR,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}

@inproceedings{wang2018deep,
  title={Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision},
  author={Wang, Hai and Poon, Hoifung},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1891--1902},
  year={2018}
}

@article{tian2021understanding,
  title={Understanding self-supervised learning dynamics without contrastive pairs},
  author={Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  journal={ICML},
  year={2021}
}

@inproceedings{ng2020ssmba,
    title = {SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness},
    author = {Nathan Ng and Kyunghyun Cho and Marzyeh Ghassemi},
    booktitle = {Proc. of EMNLP},
    year = {2020},
    url = {https://arxiv.org/abs/2009.10195}
}
@article{lemos2022rediscovering,
  title={Rediscovering orbital mechanics with machine learning},
  author={Lemos, Pablo and Jeffrey, Niall and Cranmer, Miles and Ho, Shirley and Battaglia, Peter},
  journal={arXiv preprint arXiv:2202.02306},
  year={2022}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{matheson1976crps,
 author = {James E. Matheson and Robert L. Winkler},
 journal = {Management Science},
 number = {10},
 pages = {1087--1096},
 publisher = {INFORMS},
 title = {Scoring Rules for Continuous Probability Distributions},
 volume = {22},
 year = {1976}
}

@article{fortin2014ssr,
      author = "V.  Fortin and M.  Abaza and F.  Anctil and R.  Turcotte",
      title = "Why Should Ensemble Spread Match the RMSE of the Ensemble Mean?",
      journal = "Journal of Hydrometeorology",
      year = "2014",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "15",
      number = "4",
      doi = "https://doi.org/10.1175/JHM-D-14-0008.1",
      pages=      "1708 - 1713",
      url = "https://journals.ametsoc.org/view/journals/hydr/15/4/jhm-d-14-0008_1.xml"
}


@misc{unterthiner2019fvd,
    title={{FVD}: A new Metric for Video Generation},
    author={Thomas Unterthiner and Sjoerd van Steenkiste and Karol Kurach and Rapha{\"e}l Marinier and Marcin Michalski and Sylvain Gelly},
    year={2019},
    url={https://openreview.net/forum?id=rylgEULtdN},
    journal={ICLR  Workshop for Deep Generative Models for Highly Structured Data},
}

%%%%%%%%%%%%%%%% Diffusion models
@misc{ling2022diffusionsurvey,
  url = {https://arxiv.org/abs/2209.00796},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
  year = {2022},
}

@inproceedings{ho2020ddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 year = {2020}
}


@InProceedings{sohldickstein2015deepunsupervised,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  year = 	 {2015},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@inproceedings{karras2022edm,
  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle = {Proc. NeurIPS},
  year      = {2022}
}

@article{song2017pixeldefend,
  title   = {Pixeldefend: Leveraging generative models to understand and defend against adversarial examples},
  author  = {Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
  journal = {arXiv preprint arXiv:1710.10766},
  year    = {2017}
}
@inproceedings{song2019generative,
  title     = {Generative modeling by estimating gradients of the data distribution},
  author    = {Song, Yang and Ermon, Stefano},
  booktitle = NIPS,
  volume    = {32},
  year      = {2019}
}
@inproceedings{song2019sliced,
  author    = {Yang Song and
               Sahaj Garg and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {204},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/204.pdf}
}
@inproceedings{song2021ddim,
  title     = {Denoising Diffusion Implicit Models},
  author    = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}
@inproceedings{song2020improved,
  title     = {Improved techniques for training score-based generative models},
  author    = {Song, Yang and Ermon, Stefano},
  booktitle = NIPS,
  volume    = {33},
  pages     = {12438--12448},
  year      = {2020}
}
@inproceedings{song2020scoreSDE,
  title     = {Score-Based Generative Modeling through Stochastic Differential Equations},
  author    = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}
@inproceedings{song2021maximum,
  title     = {Maximum likelihood training of score-based diffusion models},
  author    = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  booktitle = NIPS,
  volume    = {34},
  pages     = {1415--1428},
  year      = {2021}
}
@inproceedings{song2021solving,
  title     = {Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},
  author    = {Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}
@inproceedings{lou2023reflected,
      title={Reflected Diffusion Models},
      author={Aaron Lou and Stefano Ermon},
      booktitle={International Conference on Machine Learning},
      year={2023},
}
@article{song2021train,
  title   = {How to train your energy-based models},
  author  = {Song, Yang and Kingma, Diederik P},
  journal = {arXiv preprint arXiv:2101.03288},
  year    = {2021}
}
@article{song2022applying,
  title   = {Applying Regularized {S}chr{ö}dinger-Bridge-Based Stochastic Process in Generative Modeling},
  author  = {Song, Ki-Ung},
  journal = {arXiv preprint arXiv:2208.07131},
  year    = {2022}
}


%%%%%%%%%%%%%%%%%%%% Image text diffusion models
@article{saharia2022image,
  title     = {Image super-resolution via iterative refinement},
  author    = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J and Norouzi, Mohammad},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2022},
  publisher = {IEEE}
}
@inproceedings{saharia2022palette,
  title     = {Palette: Image-to-image diffusion models},
  author    = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris and Ho, Jonathan and Salimans, Tim and Fleet, David and Norouzi, Mohammad},
  booktitle = {Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings},
  pages     = {1--10},
  year      = {2022}
}
@article{saharia2022imagen,
  title   = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author  = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal=NIPS,
  year    = {2022}
}

@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}
@inproceedings{radford2021learning,
  title     = {Learning transferable visual models from natural language supervision},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle = ICML,
  pages     = {8748--8763},
  year      = {2021}
}
@inproceedings{ramesh2021zero,
  title     = {Zero-shot text-to-image generation},
  author    = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = ICML,
  pages     = {8821--8831},
  year      = {2021}
}
@article{ramesh2022dalle2,
  title   = {Hierarchical text-conditional image generation with clip latents},
  author  = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal = {arXiv preprint arXiv:2204.06125},
  year    = {2022}
}

@inproceedings{rombach2022stablediffusion,
  title     = {High-resolution image synthesis with latent diffusion models},
  author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle = CVPR,
  pages     = {10684--10695},
  year      = {2022}
}

%%%%%%%%%%% Video diffusion models
@article{voleti2022mcvd,
  url = {https://arxiv.org/abs/2205.09853},
  author = {Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Christopher},
  title = {{MCVD}: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation},
  journal=NIPS,
  year = {2022},
}

@article{ho2022videodiffusion,
    title={Video diffusion models},
    author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
    journal=NIPS,
    year={2022}}
}

@article{singer2022makeavideo,
  url = {https://arxiv.org/abs/2209.14792},
  author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},
  year = {2022},
}

@article{ho2022imagenvideo,
  url = {https://arxiv.org/abs/2210.02303},
  author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
  title = {Imagen Video: High Definition Video Generation with Diffusion Models},
  year = {2022},
}
@article{harvey2022flexiblevideos,
  title   = {Flexible Diffusion Modeling of Long Videos},
  author  = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  journal = NIPS,
  year    = {2022}
}

@article{yang2022diffusion,
  title   = {Diffusion probabilistic modeling for video generation},
  author  = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
  journal = {arXiv preprint arXiv:2203.09481},
  year    = {2022}
}

%%%%%%% DL For TIMESERIES
@inproceedings{Rasul2021AutoregressiveDD,
  title={Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
  author={Kashif Rasul and Calvin Seward and Ingmar Schuster and Roland Vollgraf},
  booktitle={ICML},
  year={2021}
}

@article{lopez2023diffusionimputation,
    title={Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models},
    author={Juan Lopez Alcaraz and Nils Strodthoff},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=hHiIbk7ApW},
}

@inproceedings{wu2021quantifying,
  title={Quantifying uncertainty in deep spatiotemporal forecasting},
  author={Wu, Dongxia and Gao, Liyao and Chinazzi, Matteo and Xiong, Xinyue and Vespignani, Alessandro and Ma, Yi-An and Yu, Rose},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1841--1851},
  year={2021}
}

%%%%%%%%%%% NOISING METHOD FOR DIFFUSION
@article{bansal2022cold,
      title={Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
      author={Arpit Bansal and Eitan Borgnia and Hong-Min Chu and Jie S. Li and Hamid Kazemi and Furong Huang and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2023},
      journal = NIPS,
}

@article{daras2022softdiffusion,
  url = {https://arxiv.org/abs/2209.05442},
  author = {Daras, Giannis and Delbracio, Mauricio and Talebi, Hossein and Dimakis, Alexandros G. and Milanfar, Peyman},
  title = {Soft Diffusion: Score Matching for General Corruptions},
  publisher = {arXiv},
  year = {2022},
}

@article{Deasy2021HeavytailedDS,
  title={Heavy-tailed denoising score matching},
  author={Deasy, Jacob and Simidjievski, Nikola and Li{\`o}, Pietro},
  journal={arXiv preprint arXiv:2112.09788},
  year={2021}
}


@inproceedings{Hoogeboom2022EquivariantDF,
  title={Equivariant diffusion for molecule generation in 3d},
  author={Hoogeboom, Emiel and Satorras, Victor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={8867--8887},
  year={2022},
  organization={PMLR}
}

@inproceedings{
Hoogeboom2021AutoregressiveDM,
title={Autoregressive Diffusion Models},
author={Emiel Hoogeboom and Alexey A. Gritsenko and Jasmijn Bastings and Ben Poole and Rianne van den Berg and Tim Salimans},
booktitle={International Conference on Learning Representations},
year={2022},
}


@article{nachmani2021denoising,
  title={Denoising diffusion gamma models},
  author={Nachmani, Eliya and Roman, Robin San and Wolf, Lior},
  journal={arXiv preprint arXiv:2110.05948},
  year={2021}
}


@article{Johnson2021BeyondIC,
  title={Beyond in-place corruption: Insertion and deletion in denoising probabilistic models},
  author={Johnson, Daniel D and Austin, Jacob and Berg, Rianne van den and Tarlow, Daniel},
  journal={arXiv preprint arXiv:2107.07675},
  year={2021}
}


@inproceedings{Avrahami2021BlendedDF,
  title={Blended diffusion for text-driven editing of natural images},
  author={Avrahami, Omri and Lischinski, Dani and Fried, Ohad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18208--18218},
  year={2022}
}

@article{lee2022progressive,
  title={Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis},
  author={Lee, Sangyun and Chung, Hyungjin and Kim, Jaehyeon and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2207.11192},
  year={2022}
}

@article{ye2022hitting,
  title={First Hitting Diffusion Models},
  author={Ye, Mao and Wu, Lemeng and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.01170},
  year={2022}
}

@inproceedings{rissanen2023generative,
    title={Generative Modelling with Inverse Heat Dissipation},
    author={Severi Rissanen and Markus Heinonen and Arno Solin},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
}

@inproceedings{hoogeboom2023blurring,
    title={Blurring Diffusion Models},
    author={Emiel Hoogeboom and Tim Salimans},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023}
}
    % url={https://openreview.net/forum?id=OjDkC57x5sz}

@article{xu2023pfgmplusplus,
  url = {https://arxiv.org/abs/2302.04265},
  author = {Xu, Yilun and Liu, Ziming and Tian, Yonglong and Tong, Shangyuan and Tegmark, Max and Jaakkola, Tommi},
  title = {PFGM++: Unlocking the Potential of Physics-Inspired Generative Models},
    booktitle={International Conference on Machine Learning},
  year = {2023},
}

%%%%%%%%%%%% DATA
@article{Rasp2020weatherbench,
	url = {https://doi.org/10.1029\%2F2020ms002203},
	year = 2020,
	author = {Stephan Rasp and Peter D. Dueben and Sebastian Scher and Jonathan A. Weyn and Soukayna Mouatadid and Nils Thuerey},
	title = {{WeatherBench}: A Benchmark Data Set for Data-Driven Weather Forecasting},
	journal = {Journal of Advances in Modeling Earth Systems}
}
@article{otness21nnbenchmark,
  title={An Extensible Benchmark Suite for Learning to Simulate Physical Systems},
  author={Karl Otness and Arvi Gjoka and Joan Bruna and Daniele Panozzo and Benjamin Peherstorfer and Teseo Schneider and Denis Zorin},
  year={2021},
  url={https://arxiv.org/abs/2108.07799},
  journal={Advances in Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks}
}

@article{huang2021oisstv2,
      author = "Boyin Huang and Chunying Liu and Viva Banzon and Eric Freeman and Garrett Graham and Bill Hankins and Tom Smith and Huai-Min Zhang",
      title = "Improvements of the Daily Optimum Interpolation Sea Surface Temperature (DOISST) Version 2.1",
      journal = "Journal of Climate",
      year = "2021",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "34",
      number = "8",
      doi = "https://doi.org/10.1175/JCLI-D-20-0166.1",
      pages=      "2923 - 2939",
      url = "https://journals.ametsoc.org/view/journals/clim/34/8/JCLI-D-20-0166.1.xml"
}

%  Deep learning weather forecasting
@article{pathak2022fourcastnet,
  url = {https://arxiv.org/abs/2202.11214},
  author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
  title = {{FourCastNet}: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators},
  year = {2022},
}   % At short term similarly good or better than IFS. Training is done on next-step predictions, and then fine-tuned on unrolled step 1 and 2 predictions

@article{lam2022graphcast,
  url = {https://arxiv.org/abs/2212.12794},
  author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Stott, Jacklynn and Vinyals, Oriol and Mohamed, Shakir and Battaglia, Peter},
  title = {{GraphCast}: Learning skillful medium-range global weather forecasting},
  year = {2022},
}

@article{bi2022pangu,
  title={Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={arXiv preprint arXiv:2211.02556},
  year={2022}
} % Hierarchical lead-time models so that mid-range forecasts require few NN calls. Ensembling via input perturbations like fourcastnet

@article{nguyen2023climax,
  title={Clima{X}: A foundation model for weather and climate},
  author={Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K and Grover, Aditya},
  journal={International Conference on Machine Learning},
  year={2023}
}

@inproceedings{de2018physicalsstbaseline,
    title={Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge},
    author={Emmanuel de B\'{e}zenac and Arthur Pajot and Patrick Gallinari},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=By4HsfWAZ},
}

@inproceedings{bezenac2020normalizing,
 author = {de B\'{e}zenac, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and Bohlke-Schneider, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2995--3007},
 title = {Normalizing Kalman Filters for Multivariate Time Series Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1f47cef5e38c952f94c5d61726027439-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{sanchezgonzalez2020learning,
  title={Learning to Simulate Complex Physics with Graph Networks},
  author={Alvaro Sanchez-Gonzalez and
          Jonathan Godwin and
          Tobias Pfaff and
          Rex Ying and
          Jure Leskovec and
          Peter W. Battaglia},
  booktitle={International Conference on Machine Learning},
  year={2020}
}  % Add noise to the training data to attain stable rollouts (so that input distribution more closely to error-prone rollout distributions). The model predicts and is trained for next-step prediction only.




@article{kochkov2021mlfluids,
    author = {Dmitrii Kochkov  and Jamie A. Smith  and Ayya Alieva  and Qing Wang  and Michael P. Brenner  and Stephan Hoyer },
    title = {Machine learning–accelerated computational fluid dynamics},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {118},
    number = {21},
    pages = {e2101784118},
    year = {2021},
    doi = {10.1073/pnas.2101784118},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2101784118},
}


@inproceedings{graubner2022calibration,
  title={Calibration of Large Neural Weather Models},
  author={Graubner, Andre and Kamyar Azizzadenesheli, Kamyar and Pathak, Jaideep and Mardani, Morteza and Pritchard, Mike and Kashinath, Karthik and Anandkumar, Anima},
  booktitle={NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning},
  year={2022}
}
@inproceedings{han2022predicting,
    title={Predicting Physics in Mesh-reduced Space with Temporal Attention},
    author={Xu Han and Han Gao and Tobias Pfaff and Jian-Xun Wang and Liping Liu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=XctLdNfCmP}
}  % Maps snapshots to latent (graph-based) space (basically to latent tokens), on which an autoregressive transformer operates to evolve the dynamics up to T snapshots into the future (using the previously predicted tokens as input/query). Interesting is that they propose a multi-step loss curriculum, i.e the model "is trained by incrementally including loss terms computed from different time steps." (i.e. it seems, first only loss for x1, then x1 and x2, then x1 x2 x3, etc.). Issue: Needs to attend during training on whole simulation trajectories.

@inproceedings{brandstetter2022message,
    title={Message Passing Neural {PDE} Solvers},
    author={Johannes Brandstetter and Daniel E. Worrall and Max Welling},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=vSix3HPYKSU}
} % Basically, does autoregressive rollouts but over direct multi-step predictions ("temporal bundling") and adds a stability loss term on rolled out predictions (two steps in exps) based on a domain adaptation adversarial loss

@inproceedings{brandstetter2023clifford,
    title={Clifford Neural Layers for {PDE} Modeling},
    author={Johannes Brandstetter and Rianne van den Berg and Max Welling and Jayesh K Gupta},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=okwxL_c4x84}
}
@inproceedings{janny2023eagle,
    title={{EAGLE}: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers},
    author={Steeven Janny and Aur{\'e}lien B{\'e}n{\'e}teau and Madiha Nadri and Julie Digne and Nicolas Thome and Christian Wolf},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=mfIX4QpsARJ}
}

@article{wang2020tfnet,
   title={Towards Physics-informed Deep Learning for Turbulent Flow Prediction},
   author={Rui Wang and Karthik Kashinath and Mustafa Mustafa and Adrian Albert and Rose Yu},
   journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
   year = {2020}
}
% To generate multi-step forecasts, \cite{wang2020tfnet} perform one-step ahead prediction and roll out the predictions autoregressively

@inproceedings{wang2022metalearning,
      title={Meta-Learning Dynamics Forecasting Using Task Inference},
      author={Rui Wang and Robin Walters and Rose Yu},
      year={2022},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{wan2023evolve,
    title={Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics For Advection-Dominated Systems},
    author={Zhong Yi Wan and Leonardo Zepeda-Nunez and Anudhyan Boral and Fei Sha},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=Z4s73sJYQM}
}

@inproceedings{tran2023factorized,
    title={Factorized Fourier Neural Operators},
    author={Alasdair Tran and Alexander Mathews and Lexing Xie and Cheng Soon Ong},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=tmIiMPl4IPa}
}

@inproceedings{zhang2018mixup,
    title={mixup: Beyond Empirical Risk Minimization},
    author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2018},
    url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@article{tu2022ccaiautoml,
  url = {https://arxiv.org/abs/2210.03324},
  author = {Tu, Renbo and Roberts, Nicholas and Prasad, Vishak and Nayak, Sibasis and Jain, Paarth and Sala, Frederic and Ramakrishnan, Ganesh and Talwalkar, Ameet and Neiswanger, Willie and White, Colin},
  title = {AutoML for Climate Change: A Call to Action},
  year = {2022},
}

@Article{liu2022convnext,
  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},
  title   = {A ConvNet for the 2020s},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2022},
}

%%%%%%%%%%%% DISCRIMINATIVE MODELS FOR CLIMATE DATA


@misc{compressingweatherbench,
  doi = {10.48550/ARXIV.2210.12538},

  url = {https://arxiv.org/abs/2210.12538},

  author = {Huang, Langwen and Hoefler, Torsten},

  keywords = {Machine Learning (cs.LG), Information Theory (cs.IT), Atmospheric and Oceanic Physics (physics.ao-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},

  title = {Compressing multidimensional weather and climate data into neural networks},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@article{drydenspatial,
  title={Spatial Mixture-of-Experts},
  author={Dryden, Nikoli and Hoefler, Torsten}
}


%%%%%%%%%%%% GENERATIVE MODELS FOR CLIMATE DATA

@article{harris2022generative,
  title={A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts},
  author={Harris, Lucy and McRae, Andrew TT and Chantry, Matthew and Dueben, Peter D and Palmer, Tim N},
  journal={arXiv preprint arXiv:2204.02028},
  year={2022}
}

@article{ravuri2021skilful,
  title={Skilful precipitation nowcasting using deep generative models of radar},
  author={Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and others},
  journal={Nature},
  volume={597},
  number={7878},
  pages={672--677},
  year={2021},
  publisher={Nature Publishing Group},
  url={https://doi.org/10.1038/s41586-021-03854-z}
}
% ravuri2021skilful is based on multi-step predicting GANs
% Note on their Unet baseline: "rather than  predicting only a single output and using autoregressive sampling
% during evaluation, the model predicts all frames in a single forward pass. This somewhat mitigates the excessive blurring found in ref. 5 and improves results on quantitative evaluation"


@article{sonderby2020metnet,
  title={Metnet: A neural weather model for precipitation forecasting},
  author={S{\o}nderby, Casper Kaae and Espeholt, Lasse and Heek, Jonathan and Dehghani, Mostafa and Oliver, Avital and Salimans, Tim and Agrawal, Shreya and Hickey, Jason and Kalchbrenner, Nal},
  journal={arXiv preprint arXiv:2003.12140},
  year={2020}
}

@article{espeholt2022metnet2,
    author={Espeholt, Lasse and Agrawal, Shreya and S{\o}nderby, Casper and Kumar, Manoj and Heek, Jonathan and Bromberg, Carla
    and Gazen, Cenk and Carver, Rob and Andrychowicz, Marcin and Hickey, Jason and Bell, Aaron and Kalchbrenner, Nal},
    title={Deep learning for twelve hour precipitation forecasts},
    journal={Nature Communications},
    year={2022},
    month={Sep},
    day={01},
    volume={13},
    number={1},
    pages={5145},
    issn={2041-1723},
    doi={10.1038/s41467-022-32483-x},
    url={https://doi.org/10.1038/s41467-022-32483-x}
}



%%%%%%%%%%%%%%%%% OTHER

@article{bauer2015thequiet,
    author={Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
    title={The quiet revolution of numerical weather prediction},
    journal={Nature},
    year={2015},
    month={Sep},
    day={01},
    volume={525},
    number={7567},
    pages={47-55},
    issn={1476-4687},
    doi={10.1038/nature14956},
    url={https://doi.org/10.1038/nature14956}
}


@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{matheson1976scoring,
  title={Scoring rules for continuous probability distributions},
  author={Matheson, James E and Winkler, Robert L},
  journal={Management science},
  volume={22},
  number={10},
  pages={1087--1096},
  year={1976},
  publisher={INFORMS}
}

@article{hersbach2000decomposition,
  title={Decomposition of the continuous ranked probability score for ensemble prediction systems},
  author={Hersbach, Hans},
  journal={Weather and Forecasting},
  volume={15},
  number={5},
  pages={559--570},
  year={2000},
  publisher={American Meteorological Society}
}

@inproceedings{unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{cnn,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}



% Multi step forecasting
@inproceedings{lambert21longtermRL,
  author={Lambert, Nathan and Wilcox, Albert and Zhang, Howard and Pister, Kristofer S. J. and Calandra, Roberto},
  title={Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning},
  booktitle={IEEE Conference on Decision and Control (CDC)},
  year={2021}
}


@article{ham2019dlenso,
    author = {Ham, Yoo-Geun and Kim, Jeong-Hwan and Luo, Jing-Jia},
    year = 2019,
    month = 9,
    title = {Deep learning for multi-year ENSO forecasts},
    journal = {Nature},
    pages = "568-572",
    volume = 573,
    issue = 7775,
}

@article{vos2021long,
  title={Long-range seasonal forecasting of 2m-temperature with machine learning},
  author={Vos, Etienne E and Gritzman, Ashley and Makhanya, Sibusisiwe and Mashinini, Thabang and Watson, Campbell D},
  journal={NeurIPS, Tackling Climate Change with Machine Learning},
  year={2020}
}




@article{raissi2017pidl,
  url = {https://arxiv.org/abs/1711.10561},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  title = {Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},
  year = {2017},
}


@article{kashinath2021piml,
    author = {Kashinath, K.  and Mustafa, M.  and Albert, A.  and Wu, J-L.  and Jiang, C.  and Esmaeilzadeh, S.  and Azizzadenesheli, K.  and Wang, R.  and Chattopadhyay, A.  and Singh, A.  and Manepalli, A.  and Chirila, D.  and Yu, R.  and Walters, R.  and White, B.  and Xiao, H.  and Tchelepi, H. A.  and Marcus, P.  and Anandkumar, A.  and Hassanzadeh, P.  and Prabhat, null },
    title = {Physics-informed machine learning: case studies for weather and climate modelling},
    journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
    volume = {379},
    number = {2194},
    pages = {20200093},
    year = {2021},
    doi = {10.1098/rsta.2020.0093},
    URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0093},
}

@article{brenowitz2018prognostic,
    author = {Brenowitz, N. D. and Bretherton, C. S.},
    title = {Prognostic Validation of a Neural Network Unified Physics Parameterization},
    journal = {Geophysical Research Letters},
    volume = {45},
    number = {12},
    pages = {6289-6298},
    year = {2018}
}

@article{scher2018toward,
    author = {Scher, S.},
    title = {Toward Data-Driven Weather and Climate Forecasting: Approximating a Simple General Circulation Model With Deep Learning},
    journal = {Geophysical Research Letters},
    volume = {45},
    number = {22},
    pages = {12,616-12,622},
    keywords = {machine learning, weather prediction, neural networks, deep learning, climate models},
    doi = {https://doi.org/10.1029/2018GL080704},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018GL080704},
    year = {2018}
}

@article{scher2019generalization,
    AUTHOR = {Scher, S. and Messori, G.},
    TITLE = {Generalization properties of feed-forward neural networks trained on Lorenz systems},
    JOURNAL = {Nonlinear Processes in Geophysics},
    VOLUME = {26},
    YEAR = {2019},
    NUMBER = {4},
    PAGES = {381--399},
    URL = {https://npg.copernicus.org/articles/26/381/2019/},
    DOI = {10.5194/npg-26-381-2019}
}

@article{weyn2019canmachines,
    author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
    title = {Can Machines Learn to Predict Weather? Using Deep Learning to Predict Gridded 500-hPa Geopotential Height From Historical Weather Data},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {11},
    number = {8},
    pages = {2680-2693},
    keywords = {machine learning, weather prediction, deep learning, neural network},
    doi = {https://doi.org/10.1029/2019MS001705},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001705},
    year = {2019}
}

@article {rasp2018postprocessing,
      author = "Stephan Rasp and Sebastian Lerch",
      title = "Neural Networks for Postprocessing Ensemble Weather Forecasts",
      journal = "Monthly Weather Review",
      year = "2018",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "146",
      number = "11",
      pages=      "3885 - 3900",
}

@article{rasp2021datadriven,
    author = {Rasp, Stephan and Thuerey, Nils},
    title = {Data-Driven Medium-Range Weather Prediction With a Resnet Pretrained on Climate Simulations: A New Model for WeatherBench},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {13},
    number = {2},
    keywords = {deep learning, machine learning, numerical weather forecasting},
    doi = {https://doi.org/10.1029/2020MS002405},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002405},
    year = {2021}
}


@inproceedings{chattopadhyay2020deep,
  title={Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence},
  author={Chattopadhyay, Ashesh and Mustafa, Mustafa and Hassanzadeh, Pedram and Kashinath, Karthik},
  booktitle={Proceedings of the 10th International Conference on Climate Informatics},
  pages={106--112},
  year={2020}
}

@article{chattopadhyay2022why,
  title={Why are deep learning-based models of geophysical turbulence long-term unstable? },
  author={Chattopadhyay, Ashesh and Hassanzadeh, Pedram},
  journal={NeurIPS, Machine Learning for Physical Sciences},
  year={2022}
}

@article{keisler2022forecasting,
  url = {https://arxiv.org/abs/2202.07575},
  author = {Keisler, Ryan},
  title = {Forecasting Global Weather with Graph Neural Networks},
  year = {2022},
}


@article{erichson2019pimlLyapunov,
  url = {https://arxiv.org/abs/1905.10866},
  author = {Erichson, N. Benjamin and Muehlebach, Michael and Mahoney, Michael W.},
  title = {Physics-informed Autoencoders for Lyapunov-stable Fluid Flow Prediction},
  year = {2019},
}

@article{mamakoukas2020learningstable,
    author = {Mamakoukas, Giorgos and Abraham, Ian and Murphey, Todd},
    year = {2020},
    journal = {IEEE Transactions on Robotics},
    title = {Learning Stable Models for Prediction and Control}
}


% Probabilistic weather forecasting
@article{scher2021ensemble,
    author = {Scher, Sebastian and Messori, Gabriele},
    title = {Ensemble Methods for Neural Network-Based Weather Forecasts},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {13},
    number = {2},
    pages = {},
    keywords = {ensemble forecasting, machine learning, neural networks, singular value decomposition, weather forecasting},
    doi = {https://doi.org/10.1029/2020MS002331},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002331},
    eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002331},
    abstract = {Abstract Ensemble weather forecasts enable a measure of uncertainty to be attached to each forecast, by computing the ensemble's spread. However, generating an ensemble with a good spread-error relationship is far from trivial, and a wide range of approaches to achieve this have been explored—chiefly in the context of numerical weather prediction models. Here, we aim to transform a deterministic neural network weather forecasting system into an ensemble forecasting system. We test four methods to generate the ensemble: random initial perturbations, retraining of the neural network, use of random dropout in the network, and the creation of initial perturbations with singular vector decomposition. The latter method is widely used in numerical weather prediction models, but is yet to be tested on neural networks. The ensemble mean forecasts obtained from these four approaches all beat the unperturbed neural network forecasts, with the retraining method yielding the highest improvement. However, the skill of the neural network forecasts is systematically lower than that of state-of-the-art numerical weather prediction models.},
    year = {2021}
}
% scher2021ensemble studies 4 methods to make a deterministic NN probabilistic (initial conditions, inference dropout, and random seeds). All models are next-step (=6h) forecasters, that are unrolled autoregressively

@article{garg2022weatherbenchprob,
  title={WeatherBench Probability: A benchmark dataset for probabilistic medium-range weather forecasting along with deep learning baseline models},
  author={Garg, Sagar and Rasp, Stephan and Thuerey, Nils},
  journal={arXiv preprint arXiv:2205.00865},
  year={2022}
}  % garg2022weatherbenchprob studies proba forecasts based on inference dropout, Gaussian parametric, and by discretizing into bins

@article{hu2023swinrnn,
    author = {Hu, Yuan and Chen, Lei and Wang, Zhibin and Li, Hao},
    title = {SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {15},
    number = {2},
    pages = {e2022MS003211},
    keywords = {medium-range weather forecasting, data-driven method, ensemble forecast, learned distribution perturbation},
    doi = {https://doi.org/10.1029/2022MS003211},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022MS003211},
    eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2022MS003211},
    note = {e2022MS003211 2022MS003211},
    abstract = {Abstract The data-driven approaches for medium-range weather forecasting are recently shown to be extraordinarily promising for ensemble forecasting due to their fast inference speed compared to the traditional numerical weather prediction models. However, their forecast accuracy can hardly match the state-of-the-art operational ECMWF Integrated Forecasting System (IFS) model. Previous data-driven approaches perform ensemble forecasting using some simple perturbation methods, like the initial condition perturbation and the Monte Carlo dropout. However, their ensemble performance is often limited arguably by the sub-optimal ways of applying perturbation. We propose a Swin Transformer-based Variational Recurrent Neural Network (SwinVRNN), which is a stochastic weather forecasting model combining a SwinRNN predictor with a perturbation module. SwinRNN is designed as a Swin Transformer-based recurrent neural network, which predicts the future states deterministically. Furthermore, to model the stochasticity in the prediction, we design a perturbation module following the Variational Auto-Encoder paradigm to learn the multivariate Gaussian distributions of a time-variant stochastic latent variable from the data. Ensemble forecasting can be easily performed by perturbing the model features leveraging the noise sampled from the learned distribution. We also compare four categories of perturbation methods for ensemble forecasting, that is, fixed distribution perturbation, learned distribution perturbation, MC dropout, and multi model ensemble. Comparisons on the WeatherBench data set show that the learned distribution perturbation method using our SwinVRNN model achieves remarkably improved forecasting accuracy and reasonable ensemble spread due to the joint optimization of the two targets. More notably, SwinVRNN surpasses operational IFS on the surface variables of the 2-m temperature and the 6-hourly total precipitation at all lead times up to 5 days (Code is available at https://github.com/tpys/wwprediction).},
    year = {2023}
}
% SwinRNN adds stochasticity to the deterministic RNN (next-step predictor) by perturbing a latent condition with a VAE


@article{thuemmel2023inductive,
      title={Inductive biases in deep learning models for weather prediction},
      author={Jannik Thuemmel and Matthias Karlbauer and Sebastian Otte and Christiane Zarfl and Georg Martius and Nicole Ludwig and Thomas Scholten and Ulrich Friedrich and Volker Wulfmeyer and Bedartha Goswami and Martin V. Butz},
      year={2023},
      eprint={2304.04664},
      archivePrefix={arXiv},
}



@article {300BillionServed2009,
      author = "Jeffrey K. Lazo and Rebecca E. Morss and Julie L. Demuth",
      title = "300 Billion Served: Sources, Perceptions, Uses, and Values of Weather Forecasts",
      journal = "Bulletin of the American Meteorological Society",
      year = "2009",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "90",
      number = "6",
      doi = "10.1175/2008BAMS2604.1",
      pages=      "785 - 798",
      url = "https://journals.ametsoc.org/view/journals/bams/90/6/2008bams2604_1.xml"
}

@article{gneiting2005weather,
    author = {Tilmann Gneiting  and Adrian E. Raftery },
    title = {Weather Forecasting with Ensemble Methods},
    journal = {Science},
    volume = {310},
    number = {5746},
    pages = {248-249},
    year = {2005},
    doi = {10.1126/science.1115255},
    URL = {https://www.science.org/doi/abs/10.1126/science.1115255},
}

@article{gneiting2014Probabilistic,
    author = {Gneiting, Tilmann and Katzfuss, Matthias},
    title = {Probabilistic Forecasting},
    journal = {Annual Review of Statistics and Its Application},
    volume = {1},
    number = {1},
    pages = {125-151},
    year = {2014}
}



@article{bevacqua2023smiles,
  title    = "Advancing research on compound weather and climate events via
              large ensemble model simulations",
  author   = "Bevacqua, Emanuele and Suarez-Gutierrez, Laura and
              J{\'e}z{\'e}quel, Agla{\'e} and Lehner, Flavio and Vrac, Mathieu
              and Yiou, Pascal and Zscheischler, Jakob",
  abstract = "Societally relevant weather impacts typically result from
              compound events, which are rare combinations of weather and
              climate drivers. Focusing on four event types arising from
              different combinations of climate variables across space and
              time, here we illustrate that robust analyses of compound events
              --- such as frequency and uncertainty analysis under present-day
              and future conditions, event attribution to climate change, and
              exploration of low-probability-high-impact events --- require
              data with very large sample size. In particular, the required
              sample is much larger than that needed for analyses of univariate
              extremes. We demonstrate that Single Model Initial-condition
              Large Ensemble (SMILE) simulations from multiple climate models,
              which provide hundreds to thousands of years of weather
              conditions, are crucial for advancing our assessments of compound
              events and constructing robust model projections. Combining
              SMILEs with an improved physical understanding of compound events
              will ultimately provide practitioners and stakeholders with the
              best available information on climate risks.",
  journal  = "Nature Communications",
  volume   =  14,
  number   =  1,
  pages    = "2145",
  month    =  apr,
  year     =  2023
}

@article{el2021implicit,
  title={Implicit deep learning},
  author={El Ghaoui, Laurent and Gu, Fangda and Travacca, Bertrand and Askari, Armin and Tsai, Alicia},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={3},
  number={3},
  pages={930--958},
  year={2021},
  publisher={SIAM}
}


%%%%%% DYNAMICS FORECASTING APPLICATIONS %%%%%%

@article{leggett2020united,
  title={The united nations framework convention on climate change, the Kyoto protocol, and the Paris agreement: a summary},
  author={Leggett, Jane A},
  journal={UNFCC: New York, NY, USA},
  volume={2},
  year={2020}
}

@article{labadie2004optimal,
  title={Optimal operation of multireservoir systems: State-of-the-art review},
  author={Labadie, John W},
  journal={Journal of water resources planning and management},
  volume={130},
  number={2},
  pages={93--111},
  year={2004},
  publisher={American Society of Civil Engineers}
}

@article{brown2015future,
  title={The future of water resources systems analysis: Toward a scientific framework for sustainable water management},
  author={Brown, Casey M and Lund, Jay R and Cai, Ximing and Reed, Patrick M and Zagona, Edith A and Ostfeld, Avi and Hall, Jim and Characklis, Gregory W and Yu, Winston and Brekke, Levi},
  journal={Water resources research},
  volume={51},
  number={8},
  pages={6110--6124},
  year={2015},
  publisher={Wiley Online Library}
}

@article{preisler2007statistical,
  title={Statistical model for forecasting monthly large wildfire events in western United States},
  author={Preisler, Haiganoush K and Westerling, Anthony L},
  journal={Journal of Applied Meteorology and Climatology},
  volume={46},
  number={7},
  pages={1020--1030},
  year={2007}
}

@article{westerling2003climate,
  title={Climate and wildfire in the western United States},
  author={Westerling, Anthony L and Gershunov, Alexander and Brown, Timothy J and Cayan, Daniel R and Dettinger, Michael D},
  journal={Bulletin of the American Meteorological Society},
  volume={84},
  number={5},
  pages={595--604},
  year={2003},
  publisher={American Meteorological Society}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{dhariwal2021diffusion,
	title={Diffusion models beat gans on image synthesis},
	author={Dhariwal, Prafulla and Nichol, Alex},
	journal={arXiv preprint arXiv:2105.05233},
	year={2021}
}

@article{ho2021cascaded,
	title={Cascaded Diffusion Models for High Fidelity Image Generation},
	author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	year={2021}
}

@article{jalal2021robust,
	title={Robust Compressed Sensing MRI with Deep Generative Priors},
	author={Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G and Tamir, Jonathan I},
	journal={Advances in neural information processing systems},
	year={2021}
}

@inproceedings{
	song2022solving,
	title={Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},
	author={Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://arxiv.org/abs/2111.08005},
}

@article{alain2016gsns,
	title={GSNs: generative stochastic networks},
	author={Alain, Guillaume and Bengio, Yoshua and Yao, Li and Yosinski, Jason and Thibodeau-Laufer, Eric and Zhang, Saizheng and Vincent, Pascal},
	journal={Information and Inference: A Journal of the IMA},
	volume={5},
	number={2},
	pages={210--249},
	year={2016},
	publisher={Oxford University Press}
}

@article{popov2021grad,
	title={Grad-tts: A diffusion probabilistic model for text-to-speech},
	author={Popov, Vadim and Vovk, Ivan and Gogoryan, Vladimir and Sadekova, Tasnima and Kudinov, Mikhail},
	journal={arXiv preprint arXiv:2105.06337},
	year={2021}
}

@inproceedings{martens2012estimating,
	title={Estimating the hessian by back-propagating curvature},
	author={Martens, James and Sutskever, Ilya and Swersky, Kevin},
	booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
	pages={963--970},
	year={2012}
}

@inproceedings{
	jolicoeur-martineau2021adversarial,
	title={Adversarial score matching and improved sampling for image generation},
	author={Alexia Jolicoeur-Martineau and R{\'e}mi Pich{\'e}-Taillefer and Ioannis Mitliagkas and Remi Tachet des Combes},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=eLfqMl3z3lq}
}

@article{hyvarinen2005estimation,
  title={Estimation of non-normalized statistical models by score matching},
  author={Hyv{\"a}rinen, Aapo},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={695--709},
  year={2005}
}

@inproceedings{song2019generative,
	title={Generative Modeling by Estimating Gradients of the Data Distribution},
	author={Song, Yang and Ermon, Stefano},
	booktitle={Advances in Neural Information Processing Systems},
	pages={11895--11907},
	year={2019},
	url={https://arxiv.org/abs/1907.05600}
}

@book{petersen2006riemannian,
	title={Riemannian geometry},
	author={Petersen, Peter},
	volume={171},
	year={2006},
	publisher={Springer}
}

@inproceedings{ioffe2015batch,
	title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	author={Ioffe, Sergey and Szegedy, Christian},
	booktitle={International Conference on Machine Learning},
	pages={448--456},
	year={2015}
}

@inproceedings{ho2020denoising,
	author    = {Jonathan Ho and
	Ajay Jain and
	Pieter Abbeel},
	editor    = {Hugo Larochelle and
	Marc'Aurelio Ranzato and
	Raia Hadsell and
	Maria{-}Florina Balcan and
	Hsuan{-}Tien Lin},
	title     = {Denoising Diffusion Probabilistic Models},
	booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
	on Neural Information Processing Systems 2020, NeurIPS 2020, December
	6-12, 2020, virtual},
	year      = {2020},
	url       = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	timestamp = {Tue, 19 Jan 2021 15:57:09 +0100},
	biburl    = {https://dblp.org/rec/conf/nips/HoJA20.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{onuoha2018peoples,
  title={A People's Guide to AI},
  author={Onuoha, Mimi and Mother Cyborg},
  year={2018},
  publisher={Allied Media Projects}
}

@inproceedings{wu2017scalable,
	title={Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
	author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
	booktitle={Advances in neural information processing systems},
	pages={5279--5288},
	year={2017}
}

@article{bielecki2002estimation,
	title={Estimation of the Euler method error on a Riemannian manifold},
	author={Bielecki, Andrzej},
	journal={Communications in numerical methods in engineering},
	volume={18},
	number={11},
	pages={757--763},
	year={2002},
	publisher={Wiley Online Library}
}

@book{amari2007methods,
	title={Methods of information geometry},
	author={Amari, Shun-ichi and Nagaoka, Hiroshi},
	volume={191},
	year={2007},
	publisher={American Mathematical Soc.}
}

@article{amari1998natural,
	title={Natural gradient works efficiently in learning},
	author={Amari, Shun-Ichi},
	journal={Neural computation},
	volume={10},
	number={2},
	pages={251--276},
	year={1998},
	publisher={MIT Press}
}

@article{neal2001annealed,
	title={Annealed importance sampling},
	author={Neal, Radford M},
	journal={Statistics and computing},
	volume={11},
	number={2},
	pages={125--139},
	year={2001},
	publisher={Springer}
}

@inproceedings{Karras2020ada,
	title     = {Training Generative Adversarial Networks with Limited Data},
	author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
	booktitle = {Proc. NeurIPS},
	year      = {2020}
}


@inproceedings{FID,
	author    = {Martin Heusel and
	Hubert Ramsauer and
	Thomas Unterthiner and
	Bernhard Nessler and
	Sepp Hochreiter},
	editor    = {Isabelle Guyon and
	Ulrike von Luxburg and
	Samy Bengio and
	Hanna M. Wallach and
	Rob Fergus and
	S. V. N. Vishwanathan and
	Roman Garnett},
	title     = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
	Equilibrium},
	booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
	on Neural Information Processing Systems 2017, December 4-9, 2017,
	Long Beach, CA, {USA}},
	pages     = {6626--6637},
	year      = {2017},
}


@inproceedings{salimans2016weight,
	title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
	author={Salimans, Tim and Kingma, Diederik P},
	booktitle={Advances in Neural Information Processing Systems},
	pages={901--909},
	year={2016}
}

@inproceedings{neural_ode,
	author    = {Tian Qi Chen and
	Yulia Rubanova and
	Jesse Bettencourt and
	David Duvenaud},
	editor    = {Samy Bengio and
	Hanna M. Wallach and
	Hugo Larochelle and
	Kristen Grauman and
	Nicol{\`{o}} Cesa{-}Bianchi and
	Roman Garnett},
	title     = {Neural Ordinary Differential Equations},
	booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
	on Neural Information Processing Systems 2018, NeurIPS 2018, December
	3-8, 2018, Montr{\'{e}}al, Canada},
	pages     = {6572--6583},
	year      = {2018},
}


@article{ba2016layer,
	title={Layer normalization},
	author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
	journal={arXiv preprint arXiv:1607.06450},
	year={2016}
}

@inproceedings{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  booktitle={Advances in neural information processing systems},
  pages={3320--3328},
  year={2014}
}

@article{nguyen2016plug,
  title={Plug \& play generative networks: Conditional iterative generation of images in latent space},
  author={Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
  journal={arXiv preprint arXiv:1612.00005},
  year={2016}
}

@inproceedings{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2226--2234},
  year={2016}
}

@inproceedings{che2016mode,
  title={Mode Regularized Generative Adversarial Networks},
  author={Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{hjelm2017boundary,
  title={Boundary-Seeking Generative Adversarial Networks},
  author={Hjelm, R Devon and Jacob, Athul Paul and Che, Tong and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1702.08431},
  year={2017}
}

@article{roeder2020linear,
	title={On linear identifiability of learned representations},
	author={Roeder, Geoffrey and Metz, Luke and Kingma, Diederik P},
	journal={arXiv preprint arXiv:2007.00810},
	year={2020}
}

@article{kadkhodaie2020solving,
	title={Solving linear inverse problems using the prior implicit in a denoiser},
	author={Kadkhodaie, Zahra and Simoncelli, Eero P},
	journal={arXiv preprint arXiv:2007.13640},
	year={2020}
}

@inproceedings{
	grathwohl2018scalable,
	title={Scalable Reversible Generative Models with Free-form Continuous Dynamics},
	author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=rJxgknCcK7},
}

@inproceedings{song2020improved,
	author    = {Yang Song and Stefano Ermon},
	editor    = {Hugo Larochelle and
	Marc'Aurelio Ranzato and
	Raia Hadsell and
	Maria{-}Florina Balcan and
	Hsuan{-}Tien Lin},
	title     = {Improved Techniques for Training Score-Based Generative Models},
	booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
	on Neural Information Processing Systems 2020, NeurIPS 2020, December
	6-12, 2020, virtual},
	year      = {2020},
	url = {https://arxiv.org/abs/2006.09011}
}


@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{vahdat2021score,
	title={Score-based Generative Modeling in Latent Space},
	author={Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
	journal={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2021}
}

@article{tolstikhin2017wasserstein,
  title={Wasserstein auto-encoders},
  author={Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  journal={arXiv preprint arXiv:1711.01558},
  year={2017}
}

@article{de2021diffusion,
	title={Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling},
	author={De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	journal={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2021}
}

@inproceedings{song2019sliced,
	title={Sliced score matching: A scalable approach to density and score estimation},
	author={Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
	booktitle={Uncertainty in Artificial Intelligence},
	pages={574--584},
	year={2020},
	organization={PMLR},
	url={https://arxiv.org/abs/1905.07088}
}

@inproceedings{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  booktitle={International Conference on Learning Representations},
  year={2014}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein {GAN}},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}


@inproceedings{metz2017unrolled,
	author    = {Luke Metz and
	Ben Poole and
	David Pfau and
	Jascha Sohl-Dickstein},
	title     = {Unrolled Generative Adversarial Networks},
	booktitle = {5th International Conference on Learning Representations, ICLR 2017,
	Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher = {OpenReview.net},
	year      = {2017},
	url       = {https://openreview.net/forum?id=BydrOIcle},
	timestamp = {Thu, 25 Jul 2019 14:26:00 +0200},
	biburl    = {https://dblp.org/rec/conf/iclr/MetzPPS17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{chen2016variational,
  title={Variational Lossy Autoencoder},
  author={Chen, Xi and Kingma, Diederik P and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02731},
  year={2016}
}

@article{bordes2017learning,
  title={Learning to generate samples from noise through infusion training},
  author={Bordes, Florian and Honari, Sina and Vincent, Pascal},
  journal={arXiv preprint arXiv:1703.06975},
  year={2017}
}

@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose Bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2370--2378},
  year={2016}
}


@InProceedings{chwialkowski16,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}


@article{goyal2017variational,
	title={Variational walkback: Learning a transition operator as a stochastic recurrent net},
	author={Goyal, Anirudh and Ke, Nan Rosemary and Ganguli, Surya and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1711.02282},
	year={2017}
}

@inproceedings{liu2016kernelized,
	title={A kernelized Stein discrepancy for goodness-of-fit tests},
	author={Liu, Qiang and Lee, Jason and Jordan, Michael},
	booktitle={International conference on machine learning},
	pages={276--284},
	year={2016},
	organization={PMLR}
}


@inproceedings{li2015generative,
  title={Generative moment matching networks},
  author={Li, Yujia and Swersky, Kevin and Zemel, Rich},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  pages={1718--1727},
  year={2015}
}

@inproceedings{kingma2014semi,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3581--3589},
  year={2014}
}

@article{theis2015note,
  title={A note on the evaluation of generative models},
  author={Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
  journal={arXiv preprint arXiv:1511.01844},
  year={2015}
}

@article{gulrajani2016pixelvae,
  title={PixelVAE: A Latent Variable Model for Natural Images},
  author={Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  journal={arXiv preprint arXiv:1611.05013},
  year={2016}
}

@inproceedings{sonderby2016ladder,
  title={Ladder variational autoencoders},
  author={S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3738--3746},
  year={2016}
}

@article{tran2017deep,
  title={Deep and Hierarchical Implicit Models},
  author={Tran, Dustin and Ranganath, Rajesh and Blei, David M},
  journal={arXiv preprint arXiv:1702.08896},
  year={2017}
}

@article{mohamed2016learning,
  title={Learning in implicit generative models},
  author={Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1610.03483},
  year={2016}
}

@inproceedings{chen2016infogan,
	title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
	author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2172--2180},
	year={2016}
}

@inproceedings{rezende2014stochastic,
	title={Stochastic backpropagation and approximate inference in deep generative models},
	author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	booktitle={International conference on machine learning},
	pages={1278--1286},
	year={2014},
	organization={PMLR}
}

@inproceedings{gretton2007kernel,
  title={A kernel method for the two-sample-problem},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte and Sch{\"o}lkopf, Bernhard and Smola, Alex J},
  booktitle={Advances in neural information processing systems},
  pages={513--520},
  year={2007}
}

@article{chen2016variational,
  title={Variational lossy autoencoder},
  author={Chen, Xi and Kingma, Diederik P and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02731},
  year={2016}
}

@article{bowman2015generating,
  title={Generating sentences from a continuous space},
  author={Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  journal={arXiv preprint arXiv:1511.06349},
  year={2015}
}

@inproceedings{van2016pixel,
	title={Pixel recurrent neural networks},
	author={Van Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	booktitle={International Conference on Machine Learning},
	pages={1747--1756},
	year={2016},
	organization={PMLR}
}

@article{gulrajani2016pixelvae,
  title={Pixelvae: A latent variable model for natural images},
  author={Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  journal={arXiv preprint arXiv:1611.05013},
  year={2016}
}

@article{larsen2015autoencoding,
  title={Autoencoding beyond pixels using a learned similarity metric},
  author={Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
  journal={arXiv preprint arXiv:1512.09300},
  year={2015}
}

@inproceedings{dosovitskiy2016generating,
  title={Generating images with perceptual similarity metrics based on deep networks},
  author={Dosovitskiy, Alexey and Brox, Thomas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={658--666},
  year={2016}
}

@article{higgins2016beta,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2016}
}

@article{dinh2014nice,
  title={NICE: Non-linear independent components estimation},
  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1410.8516},
  year={2014}
}

@inproceedings{bengio2014deep,
  title={Deep generative stochastic networks trainable by backprop},
  author={Bengio, Yoshua and Laufer, Eric and Alain, Guillaume and Yosinski, Jason},
  booktitle={International Conference on Machine Learning},
  pages={226--234},
  year={2014}
}

@book{brooks2011handbook,
  title={Handbook of markov chain monte carlo},
  author={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year={2011},
  publisher={CRC press}
}

@article{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  journal={arXiv preprint arXiv:1703.10593},
  year={2017}
}

@article{brooks1998general,
  title={General methods for monitoring convergence of iterative simulations},
  author={Brooks, Stephen P and Gelman, Andrew},
  journal={Journal of computational and graphical statistics},
  volume={7},
  number={4},
  pages={434--455},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{white1982maximum,
  title={Maximum likelihood estimation of misspecified models},
  author={White, Halbert},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1--25},
  year={1982},
  publisher={JSTOR}
}

@inproceedings{sohl2015deep,
	title={Deep unsupervised learning using nonequilibrium thermodynamics},
	author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	booktitle={International Conference on Machine Learning},
	pages={2256--2265},
	year={2015},
	organization={PMLR}
}

@article{lecun2006tutorial,
	title={A tutorial on energy-based learning},
	author={LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M and Huang, F},
	journal={Predicting structured data},
	volume={1},
	number={0},
	year={2006}
}

@article{song2021train,
	title={How to Train Your Energy-Based Models},
	author={Song, Yang and Kingma, Diederik P},
	journal={arXiv preprint arXiv:2101.03288},
	year={2021}
}

@inproceedings{dinh2016density,
  title={Density estimation using Real NVP},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}

@inproceedings{larochelle2011neural,
  title={The neural autoregressive distribution estimator},
  author={Larochelle, Hugo and Murray, Iain},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={29--37},
  year={2011}
}

@inproceedings{nowozin2016f,
  title={f-{GAN}: {T}raining generative neural samplers using variational divergence minimization},
  author={Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  booktitle={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{wu2016quantitative,
  title={On the Quantitative Analysis of Decoder-Based Generative Models},
  author={Wu, Yuhuai and Burda, Yuri and Salakhutdinov, Ruslan and Grosse, Roger},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{fedus2017many,
  title={Many Paths to Equilibrium: {GAN}s Do Not Need to Decrease aDivergence At Every Step},
  author={Fedus, William and Rosca, Mihaela and Lakshminarayanan, Balaji and Dai, Andrew M and Mohamed, Shakir and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1710.08446},
  year={2017}
}

####################################Rademacher references####################################

#Unweighted method we are generalizing
@article{barvinok1997approximate,
  title={Approximate counting via random optimization},
  author={Barvinok, Alexander I},
  journal={Random Structures and Algorithms},
  volume={11},
  number={2},
  pages={187--198},
  year={1997},
  publisher={John Wiley \& Sons, Inc.}
}

#exact summation #P hard
@article{valiant1979complexity,
  title={The complexity of enumeration and reliability problems},
  author={Valiant, Leslie G},
  journal={SIAM Journal on Computing},
  volume={8},
  number={3},
  pages={410--421},
  year={1979},
  publisher={SIAM}
}

@article{bach2013learning,
  title={Learning with submodular functions: A convex optimization perspective},
  author={Bach, Francis and others},
  journal={Foundations and Trends in Machine Learning},
  volume={6},
  number={2-3},
  pages={145--373},
  year={2013},
  publisher={Now Publishers, Inc.}
}

%Original Gumbel (I think)
@inproceedings{papandreou2011perturb,
  title={Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models},
  author={Papandreou, George and Yuille, Alan L},
  booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on},
  pages={193--200},
  year={2011},
  organization={IEEE}
}

@inproceedings{hazan2012partition,
    Publisher = {ACM},
    Title = {On the Partition Function and Random Maximum A-Posteriori Perturbations},
    Booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
    Author = {Tamir Hazan and Tommi S. Jaakkola},
    Year = {2012},
    Pages = {991--998}
}

@inproceedings{chakraborty2013scalable,
  title={A scalable approximate model counter},
  author={Chakraborty, Supratik and Meel, Kuldeep S and Vardi, Moshe Y},
  booktitle={International Conference on Principles and Practice of Constraint Programming},
  pages={200--216},
  year={2013},
  organization={Springer}
}

@inproceedings{ermon2013taming,
  title={Taming the curse of dimensionality: Discrete integration by hashing and optimization},
  author={Ermon, Stefano and Gomes, Carla and Sabharwal, Ashish and Selman, Bart},
  booktitle={Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
  pages={334--342},
  year={2013}
}

@article{levy2017generalizing,
  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},
  author={Levy, Daniel and Hoffman, Matthew D and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.09268},
  year={2017}
}

@article{pasarica2010adaptively,
  title={Adaptively scaling the Metropolis algorithm using expected squared jumped distance},
  author={Pasarica, Cristian and Gelman, Andrew},
  journal={Statistica Sinica},
  pages={343--364},
  year={2010},
  publisher={JSTOR}
}

@inproceedings{ranganath2014black,
  title={Black box variational inference},
  author={Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  booktitle={Artificial Intelligence and Statistics},
  pages={814--822},
  year={2014}
}

@inproceedings{salimans2015markov,
  title={Markov chain monte carlo and variational inference: Bridging the gap},
  author={Salimans, Tim and Kingma, Diederik and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={1218--1226},
  year={2015}
}

@article{burda2015importance,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}

@inproceedings{mnih2016variational,
  title={Variational inference for monte carlo objectives},
  author={Mnih, Andriy and Rezende, Danilo J},
  booktitle={International Conference on Machine Learning},
  year={2016}
}

@article{naesseth2017variational,
  title={Variational Sequential Monte Carlo},
  author={Naesseth, Christian A and Linderman, Scott W and Ranganath, Rajesh and Blei, David M},
  journal={arXiv preprint arXiv:1705.11140},
  year={2017}
}

@inproceedings{germain2015made,
	title={Made: Masked autoencoder for distribution estimation},
	author={Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	booktitle={International Conference on Machine Learning},
	pages={881--889},
	year={2015},
	organization={PMLR}
}


@article{uria2016neural,
	title={Neural autoregressive distribution estimation},
	author={Uria, Benigno and C{\^o}t{\'e}, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	journal={The Journal of Machine Learning Research},
	volume={17},
	number={1},
	pages={7184--7220},
	year={2016},
	publisher={JMLR. org}
}


@inproceedings{maddison2017filtering,
  title={Filtering Variational Objectives},
  author={Maddison, Chris J and Lawson, John and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6576--6586},
  year={2017}
}

@article{le2017auto,
  title={Auto-Encoding Sequential Monte Carlo},
  author={Le, Tuan Anh and Igl, Maximilian and Jin, Tom and Rainforth, Tom and Wood, Frank},
  journal={arXiv preprint arXiv:1705.10306},
  year={2017}
}

@inproceedings{tucker2017rebar,
  title={REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models},
  author={Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2624--2633},
  year={2017}
}


@inproceedings{jones2018recent,
  title={The Recent Large Reduction in Space Launch Cost},
  author={Jones, Harry},
  year={2018},
  organization={48th International Conference on Environmental Systems}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}


@inproceedings{bora2017compressed,
  title={Compressed sensing using generative models},
  author={Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={537--546},
  year={2017},
  organization={JMLR. org}
}

@article{candes2005decoding,
  title={Decoding by linear programming},
  author={Cand{\`e}s, Emmanuel J and Tao, Terence},
  journal={IEEE Transactions on Information Theory},
  volume={51},
  number={12},
  pages={4203--4215},
  year={2005},
  publisher={IEEE}
}


@article{candes2006robust,
  title={Robust uncertainty principles: {E}xact signal reconstruction from highly incomplete frequency information},
  author={Cand{\`e}s, Emmanuel J and Romberg, Justin and Tao, Terence},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={2},
  pages={489--509},
  year={2006},
  publisher={IEEE}
}

@article{donoho2006compressed,
  title={Compressed sensing},
  author={Donoho, David L and others},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={Citeseer}
}

@inproceedings{vincent2008extracting,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008},
  organization={ACM}
}

@article{parisi1981correlation,
	title={Correlation functions and computer simulations},
	author={Parisi, Giorgio},
	journal={Nuclear Physics B},
	volume={180},
	number={3},
	pages={378--384},
	year={1981},
	publisher={Elsevier}
}

@article{ho2020denoising,
	title={Denoising diffusion probabilistic models},
	author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	journal={arXiv preprint arXiv:2006.11239},
	year={2020}
}

@inproceedings{
	song2021denoising,
	title={Denoising Diffusion Implicit Models},
	author={Jiaming Song and Chenlin Meng and Stefano Ermon},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=St1giarCHLP}
}


@article{luhman2021knowledge,
	title={Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed},
	author={Luhman, Eric and Luhman, Troy},
	journal={arXiv e-prints},
	pages={arXiv--2101},
	year={2021}
}


@article{mittal2021symbolic,
	title={Symbolic Music Generation with Diffusion Models},
	author={Mittal, Gautam and Engel, Jesse and Hawthorne, Curtis and Simon, Ian},
	journal={arXiv preprint arXiv:2103.16091},
	year={2021}
}


@article{durkan2021maximum,
	title={Maximum Likelihood Training of Score-Based Diffusion Models},
	author={Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
	journal={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2021}
}

@article{anderson1982reverse,
	title={Reverse-time diffusion equation models},
	author={Anderson, Brian DO},
	journal={Stochastic Processes and their Applications},
	volume={12},
	number={3},
	pages={313--326},
	year={1982},
	publisher={Elsevier}
}

@article{grenander1994representations,
	title={Representations of knowledge in complex systems},
	author={Grenander, Ulf and Miller, Michael I},
	journal={Journal of the Royal Statistical Society: Series B (Methodological)},
	volume={56},
	number={4},
	pages={549--581},
	year={1994},
	publisher={Wiley Online Library}
}


@article{vincent2011connection,
	title={A connection between score matching and denoising autoencoders},
	author={Vincent, Pascal},
	journal={Neural computation},
	volume={23},
	number={7},
	pages={1661--1674},
	year={2011},
	publisher={MIT Press}
}


@article{zhang2020review,
	title={A review on deep learning in medical image reconstruction},
	author={Zhang, Hai-Miao and Dong, Bin},
	journal={Journal of the Operations Research Society of China},
	pages={1--30},
	year={2020},
	publisher={Springer}
}

@article{jolicoeur2021gotta,
	title={Gotta Go Fast When Generating Data with Score-Based Models},
	author={Jolicoeur-Martineau, Alexia and Li, Ke and Pich{\'e}-Taillefer, R{\'e}mi and Kachman, Tal and Mitliagkas, Ioannis},
	journal={arXiv preprint arXiv:2105.14080},
	year={2021}
}

@inproceedings{
	song2021scorebased,
	title={Score-Based Generative Modeling through Stochastic Differential Equations},
	author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@article{agakov2004algorithm,
  title={The IM algorithm: a variational approach to information maximization},
  author={Agakov, David Barber Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={16},
  pages={201},
  year={2004}
}

@ARTICLE{madras2018learning,
    author = "Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard",
    title = "Learning Adversarially Fair and Transferable Representations",
    month = "February",
    year = "2018",
    archivePrefix = "arXiv",
    eprint = "1802.06309",
    primaryClass = "cs.LG",
    arxivid = "1802.06309"
}

@INPROCEEDINGS{zemel2013learning,
    author = "Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia",
    title = "Learning Fair Representations",
    booktitle = "International Conference on Machine Learning",
    pages = "325--333",
    month = "February",
    year = "2013",
    language = "en",
    conference = "International Conference on Machine Learning"
}

@ARTICLE{louizos2015the,
    author = "Louizos, Christos and Swersky, Kevin and Li, Yujia and Welling, Max and Zemel, Richard",
    title = "The Variational Fair Autoencoder",
    month = "November",
    year = "2015",
    archivePrefix = "arXiv",
    eprint = "1511.00830",
    primaryClass = "stat.ML",
    arxivid = "1511.00830",
    journal = "arXiv preprint arXiv:1511.00830"
}

@inproceedings{
	kong2021diffwave,
	title={DiffWave: A Versatile Diffusion Model for Audio Synthesis},
	author={Zhifeng Kong and Wei Ping and Jiaji Huang and Kexin Zhao and Bryan Catanzaro},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=a-xFK8Ymz5J}
}

@inproceedings{ShapeGF,
	title={Learning Gradient Fields for Shape Generation},
	author={Cai, Ruojin and Yang, Guandao and Averbuch-Elor, Hadar and Hao, Zekun and Belongie, Serge and Snavely, Noah and Hariharan, Bharath},
	booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
	year={2020}
}

@article{mittal2021symbolic,
	title={Symbolic Music Generation with Diffusion Models},
	author={Mittal, Gautam and Engel, Jesse and Hawthorne, Curtis and Simon, Ian},
	journal={arXiv preprint arXiv:2103.16091},
	year={2021}
}

@inproceedings{
	chen2021wavegrad,
	title={WaveGrad: Estimating Gradients for Waveform Generation},
	author={Nanxin Chen and Yu Zhang and Heiga Zen and Ron J Weiss and Mohammad Norouzi and William Chan},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=NsMLjcFaO8O}
}

@ARTICLE{edwards2015censoring,
    author = "Edwards, Harrison and Storkey, Amos",
    title = "Censoring Representations with an Adversary",
    month = "November",
    year = "2015",
    archivePrefix = "arXiv",
    eprint = "1511.05897",
    primaryClass = "cs.LG",
    arxivid = "1511.05897",
    journal = "arXiv preprint arXiv:1511.05897"
}

